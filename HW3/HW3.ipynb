{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd; \n",
    "\n",
    "#combined data sets\n",
    "combined_data = { \n",
    "                  'train': pd.read_csv('../Football/train.csv'), \n",
    "                  'test': pd.read_csv('../Football/test.csv') \n",
    "                }\n",
    "\n",
    "#data preprocessing. Giving numerical values to some features\n",
    "replacements = [\n",
    "    {'Is_Home_or_Away': 'Home'}, {'Is_Home_or_Away': 0},\n",
    "    {'Is_Home_or_Away': 'Away'}, {'Is_Home_or_Away': 1},\n",
    "    {'Is_Opponent_in_AP25_Preseason': 'Out'}, {'Is_Opponent_in_AP25_Preseason': 0},\n",
    "    {'Is_Opponent_in_AP25_Preseason': 'In'}, {'Is_Opponent_in_AP25_Preseason': 1},\n",
    "    {'Label': 'Lose'}, {'Label': 0},\n",
    "    {'Label': 'Win'}, {'Label': 1}\n",
    "]\n",
    "\n",
    "for y in combined_data:\n",
    "    for x in range(0,len(replacements) - 1)[::2]:\n",
    "        combined_data[y] = combined_data[y].replace(replacements[x], replacements[x+1])\n",
    "\n",
    "#To both sets, I applied a numerical value to the Media and the Opponent feature\n",
    "def apply_conversion_to_values(column, new_column):\n",
    "    codes = {}\n",
    "    i = 0\n",
    "    grouped_series = combined_data['train'][column].append(combined_data['test'][column])\n",
    "    for g, _ in grouped_series.groupby(grouped_series):\n",
    "        codes[g] = i\n",
    "        i = i + 1\n",
    "    for x in combined_data:\n",
    "        combined_data[x][new_column] = combined_data[x][column].apply(lambda y: codes[y])\n",
    "    return codes\n",
    "\n",
    "media_codes = apply_conversion_to_values('Media', 'MediaCode')\n",
    "opponent_codes = apply_conversion_to_values('Opponent', 'OpponentCode')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prepare data\n",
    "import numpy as np;\n",
    "\n",
    "xTrain = combined_data['train']\n",
    "xTest = combined_data['test']\n",
    "yTrain = xTrain.iloc[:,6]\n",
    "yTest = xTest.iloc[:,6]\n",
    "\n",
    "for x in ['ID', 'Date', 'Opponent', 'Media', 'Label']:\n",
    "    xTrain = xTrain.drop(x,1)\n",
    "    xTest = xTest.drop(x,1)\n",
    "\n",
    "xTrain = np.array(xTrain)\n",
    "xTest = np.array(xTest)\n",
    "yTrain = np.array(yTrain)\n",
    "yTest = np.array(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "KNN Results:\nAccuracy: 0.83  with k = 7\nPrecision: 0.75\nRecall: 0.67\nF1 score: 0.7077464788732396\n"
    }
   ],
   "source": [
    "#KNN from Scratch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "def euclidean_distances(xTrain,xTest,k):\n",
    "    distances = -2 * xTrain@xTest.T + np.sum(xTest**2,axis=1) + np.sum(xTrain**2,axis=1)[:, np.newaxis]\n",
    "    distances[distances < 0] = 0\n",
    "    distances = distances**.5\n",
    "    indices = np.argsort(distances, 0) \n",
    "    distances = np.sort(distances,0) \n",
    "    return indices[0:k,:], distances[0:k,:]\n",
    "\n",
    "def knn(xTrain,yTrain,xTest,k=3):\n",
    "    indices, distances = euclidean_distances(xTrain,xTest,k)\n",
    "    yTrain = yTrain.flatten()\n",
    "    rows, columns = indices.shape\n",
    "    predictions = list()\n",
    "    for j in range(columns):\n",
    "        temp = list()\n",
    "        for i in range(rows):\n",
    "            cell = indices[i][j]\n",
    "            temp.append(yTrain[cell])\n",
    "        predictions.append(max(temp,key=temp.count))\n",
    "    predictions=np.array(predictions)\n",
    "    return predictions\n",
    "\n",
    "def accuracy(yTest,predictions):\n",
    "    x=yTest.flatten()==predictions.flatten()\n",
    "    grade=np.mean(x)\n",
    "    return np.round(grade,2)\n",
    "\n",
    "def precision(yTest, predictions):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for x in range(len(predictions)):\n",
    "        if predictions[x] == 1:\n",
    "            if yTest[x] == 1:\n",
    "                tp = tp+1\n",
    "            else:\n",
    "                fp = fp+1\n",
    "    return np.round(tp/(tp+fp), 2)\n",
    "def recall(yTest, predictions):\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    for x in range(len(predictions)):\n",
    "        if predictions[x] == 1:\n",
    "            if yTest[x] == 1:\n",
    "                tp = tp+1\n",
    "        else:\n",
    "            if yTest[x] == 1:\n",
    "                fn = fn+1\n",
    "    return np.round(tp/(tp+fn), 2)\n",
    "\n",
    "Ks = 10\n",
    "mean_acc = np.zeros((Ks-1))\n",
    "std_acc = np.zeros((Ks-1))\n",
    "predictions = []\n",
    "for n in range(1,Ks):\n",
    "    p = knn(xTrain,yTrain,xTest,n)\n",
    "    mean_acc[n-1] = accuracy(yTest, p)\n",
    "    std_acc[n-1] = np.std(p==yTest)/np.sqrt(p.shape[0])\n",
    "    predictions.append(p)\n",
    "\n",
    "total_precision = precision(yTest, predictions[-1])\n",
    "total_recall = recall(yTest, predictions[-1])\n",
    "\n",
    "print('KNN Results:')\n",
    "print('Accuracy:', np.round(mean_acc.max(),2), ' with k =', mean_acc.argmax()+1)\n",
    "print('Precision:', total_precision)\n",
    "print('Recall:', total_recall)\n",
    "print('F1 score:', 2*(total_recall*total_precision)/(total_recall+total_precision))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}